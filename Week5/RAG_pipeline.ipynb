{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pip setuptools wheel\n",
    "\n",
    "%pip install -U --prefer-binary \\\n",
    "    numpy PyMuPDF sentence-transformers faiss-cpu \\\n",
    "    fastapi uvicorn pydantic tiktoken \\\n",
    "    ipywidgets jupyterlab_widgets \\\n",
    "    requests beautifulsoup4\n",
    "\n",
    "%pip install -U torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "%pip install -U openai\n",
    "\n",
    "%pip install -U sentence-transformers\n",
    "\n",
    "# Sanity check: make sure the kernel sees faiss\n",
    "import sys, platform, pkgutil\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Kernel executable:\", sys.executable)\n",
    "print(\"faiss importable? ->\", pkgutil.find_loader(\"faiss\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93386c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu pymupdf fastapi uvicorn pydantic numpy tiktoken hf_xet\n",
    "\n",
    "import os, json, re\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "\n",
    "DATA_DIR = \"data/arxiv\"\n",
    "INDEX_DIR = \"artifacts\"  \n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60e430",
   "metadata": {},
   "source": [
    "## Loading PDFs and extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b298698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 PDFs: ['2508.10507v1 - Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting.pdf', '2508.10528v1 - Med-GLIP Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset.pdf', '2508.10530v1 - Diversity First Quality Later A Two-Stage Assumption for Language Model Alignment.pdf', '2508.10539v1 - Improving Value-based Process Verifier via Low-Cost Variance Reduction.pdf', '2508.10548v1 - Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards.pdf'] ...\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    doc = fitz.open(path)\n",
    "    pages = []\n",
    "    for p in doc:\n",
    "        pages.append(p.get_text(\"text\"))\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def load_corpus(pdf_dir: str = DATA_DIR) -> Dict[str, str]:\n",
    "    corpus = {}\n",
    "    for name in os.listdir(pdf_dir):\n",
    "        if name.lower().endswith(\".pdf\"):\n",
    "            fp = os.path.join(pdf_dir, name)\n",
    "            corpus[name] = extract_text_from_pdf(fp)\n",
    "    return corpus\n",
    "\n",
    "corpus = load_corpus()\n",
    "print(f\"Loaded {len(corpus)} PDFs: {list(corpus.keys())[:5]}{' ...' if len(corpus)>5 else ''}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d631e",
   "metadata": {},
   "source": [
    "## Chunking (token-based, ~512 tokens with overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97c87143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2055 from 50 PDFs\n"
     ]
    }
   ],
   "source": [
    "# Token-based chunking (~512 tokens) with fallback to character-based if tiktoken isn't available\n",
    "from typing import List\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    _enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "except Exception:\n",
    "    _enc = None\n",
    "\n",
    "def chunk_by_tokens(text: str, max_tokens: int = 512, overlap_tokens: int = 64) -> List[str]:\n",
    "    \"\"\"\n",
    "    Token-based sliding window chunker targeting ~512 tokens per chunk with ~64-token overlap.\n",
    "    Falls back to character-based (~4 chars/token heuristic) if tiktoken isn't available.\n",
    "    \"\"\"\n",
    "    if _enc is None:\n",
    "        approx = max_tokens * 4\n",
    "        overlap_chars = overlap_tokens * 4\n",
    "        clean = \" \".join(text.split())\n",
    "        chunks, start = [], 0\n",
    "        while start < len(clean):\n",
    "            end = min(start + approx, len(clean))\n",
    "            chunks.append(clean[start:end])\n",
    "            if end == len(clean): break\n",
    "            start = end - overlap_chars\n",
    "        return chunks\n",
    "\n",
    "    tokens = _enc.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        sub = tokens[start:end]\n",
    "        chunks.append(_enc.decode(sub))\n",
    "        if end == len(tokens): break\n",
    "        start = end - overlap_tokens\n",
    "    return chunks\n",
    "\n",
    "# Build chunk list with metadata using token-based chunking\n",
    "docs: List[str] = []\n",
    "metadatas: List[Dict] = []\n",
    "\n",
    "for fname, text in corpus.items():\n",
    "    for ch in chunk_by_tokens(text, max_tokens=512, overlap_tokens=64):\n",
    "        docs.append(ch)\n",
    "        metadatas.append({\"source\": fname})\n",
    "\n",
    "print(f\"Total chunks: {len(docs)} from {len(corpus)} PDFs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112c477",
   "metadata": {},
   "source": [
    "## Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0a24745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abeefc6ba03344bc92da452192fbe350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2055, 384)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    # normalize_embeddings=True pairs well with FAISS inner-product search\n",
    "    embs = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "embs = embed_texts(docs)\n",
    "embs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97d5c5",
   "metadata": {},
   "source": [
    "## Build FAISS index & persist artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4afadb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index to artifacts\\arxiv.index and chunks to artifacts\\chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "if len(docs) == 0:\n",
    "    raise RuntimeError(\"No chunks found. Add PDFs to data/arxiv/ and rerun from the top.\")\n",
    "\n",
    "dim = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # inner product on normalized vectors â‰ˆ cosine similarity\n",
    "index.add(embs)\n",
    "\n",
    "# Persist index\n",
    "faiss_path = os.path.join(INDEX_DIR, \"arxiv.index\")\n",
    "faiss.write_index(index, faiss_path)\n",
    "\n",
    "# Persist chunks (text + source) as JSONL so you can reload later\n",
    "chunks_path = os.path.join(INDEX_DIR, \"chunks.jsonl\")\n",
    "with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t, m in zip(docs, metadatas):\n",
    "        f.write(json.dumps({\"text\": t, \"source\": m[\"source\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved index to {faiss_path} and chunks to {chunks_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f944e9",
   "metadata": {},
   "source": [
    "## Create SQLite FTS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7bbde03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite FTS ready at artifacts\\rag.db\n"
     ]
    }
   ],
   "source": [
    "import sqlite3, json\n",
    "from pathlib import Path\n",
    "\n",
    "DB_PATH = Path(\"artifacts/rag.db\")\n",
    "DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con = sqlite3.connect(DB_PATH)\n",
    "con.executescript(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS documents (\n",
    "  doc_id   INTEGER PRIMARY KEY,\n",
    "  source   TEXT NOT NULL\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS chunk_meta (\n",
    "  chunk_id INTEGER PRIMARY KEY,\n",
    "  doc_id   INTEGER NOT NULL,\n",
    "  source   TEXT NOT NULL,\n",
    "  FOREIGN KEY(doc_id) REFERENCES documents(doc_id)\n",
    ");\n",
    "CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts USING fts5(\n",
    "  text,\n",
    "  chunk_id UNINDEXED,\n",
    "  doc_id   UNINDEXED,\n",
    "  tokenize='porter'\n",
    ");\n",
    "DELETE FROM documents; DELETE FROM chunk_meta; DELETE FROM chunks_fts;\n",
    "\"\"\")\n",
    "\n",
    "cur = con.cursor()\n",
    "src2id = {}\n",
    "for src in sorted({m[\"source\"] for m in metadatas}):\n",
    "    cur.execute(\"INSERT INTO documents(source) VALUES (?)\", (src,))\n",
    "    src2id[src] = cur.lastrowid\n",
    "\n",
    "for cid, (t, m) in enumerate(zip(docs, metadatas)):\n",
    "    s = m[\"source\"]; did = src2id[s]\n",
    "    cur.execute(\"INSERT INTO chunk_meta(chunk_id, doc_id, source) VALUES (?,?,?)\", (cid, did, s))\n",
    "    cur.execute(\"INSERT INTO chunks_fts(rowid, text, chunk_id, doc_id) VALUES (?,?,?,?)\", (cid, t, cid, did))\n",
    "\n",
    "con.commit(); con.close()\n",
    "print(\"SQLite FTS ready at\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd4c312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_and_chunks(index_dir: str = INDEX_DIR) -> Tuple[faiss.Index, List[str], List[Dict]]:\n",
    "    idx = faiss.read_index(os.path.join(index_dir, \"arxiv.index\"))\n",
    "    dcs, metas = [], []\n",
    "    with open(os.path.join(index_dir, \"chunks.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            dcs.append(rec[\"text\"])\n",
    "            metas.append({\"source\": rec[\"source\"]})\n",
    "    return idx, dcs, metas\n",
    "\n",
    "# Example usage (uncomment to test reloading):\n",
    "# index, docs, metadatas = load_index_and_chunks()\n",
    "# print(len(docs), \"chunks reloaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c28bcb",
   "metadata": {},
   "source": [
    "## Search helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bf68cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from collections import defaultdict\n",
    "\n",
    "ART_DIR = Path(INDEX_DIR) if 'INDEX_DIR' in globals() else Path(\"artifacts\")\n",
    "DB_PATH = ART_DIR / \"rag.db\"\n",
    "\n",
    "# ----------------------------- Dense (FAISS / fallback) -----------------------------\n",
    "def dense_search(query: str, k: int = 5):\n",
    "    \"\"\"Top-k by vector similarity (embeddings are normalized â†’ dot == cosine).\"\"\"\n",
    "    if 'embed_texts' not in globals():\n",
    "        raise RuntimeError(\"embed_texts() is not defined. Run the embeddings cell first.\")\n",
    "    if 'index' not in globals():\n",
    "        raise RuntimeError(\"Vector index not found. Build/reload the FAISS (or fallback) index.\")\n",
    "\n",
    "    q = embed_texts([query])\n",
    "    kk = min(k, len(docs))\n",
    "    if kk <= 0:\n",
    "        return []\n",
    "\n",
    "    D, I = index.search(q, kk)\n",
    "    out = []\n",
    "    for rnk in range(len(I[0])):\n",
    "        idx = int(I[0][rnk])\n",
    "        out.append({\n",
    "            \"score\": float(D[0][rnk]),     # higher is better\n",
    "            \"text\":  docs[idx],\n",
    "            \"source\": metadatas[idx][\"source\"],\n",
    "            \"chunk_id\": idx,\n",
    "            \"algo\": \"dense\"\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# Backward-compat alias:\n",
    "search = dense_search\n",
    "\n",
    "# ----------------------------- Keyword (SQLite FTS5 BM25) ---------------------------\n",
    "def keyword_search(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Full-text search over chunks via SQLite FTS5.\n",
    "    Returns 'score' as a higher-is-better value (= -bm25), and includes 'kw_bm25'.\n",
    "    \"\"\"\n",
    "    if not DB_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Keyword index not found at {DB_PATH}. Run the SQLite FTS cell to create it.\")\n",
    "\n",
    "    con = sqlite3.connect(DB_PATH)\n",
    "    con.row_factory = sqlite3.Row\n",
    "    try:\n",
    "        rows = con.execute(\"\"\"\n",
    "            SELECT c.rowid AS chunk_id, cm.source, c.text, bm25(chunks_fts) AS bm25\n",
    "            FROM chunks_fts c\n",
    "            JOIN chunk_meta cm ON cm.chunk_id = c.rowid\n",
    "            WHERE chunks_fts MATCH ?\n",
    "            ORDER BY bm25 ASC\n",
    "            LIMIT ?\n",
    "        \"\"\", (query, k)).fetchall()\n",
    "    except sqlite3.OperationalError:\n",
    "        # Fallback if bm25() or fts5 extension is unavailable\n",
    "        rows = con.execute(\"\"\"\n",
    "            SELECT c.rowid AS chunk_id, cm.source, c.text\n",
    "            FROM chunks_fts c\n",
    "            JOIN chunk_meta cm ON cm.chunk_id = c.rowid\n",
    "            WHERE chunks_fts MATCH ?\n",
    "            LIMIT ?\n",
    "        \"\"\", (query, k)).fetchall()\n",
    "        out = [{\n",
    "            \"score\": 0.0,\n",
    "            \"kw_bm25\": None,\n",
    "            \"text\": r[\"text\"],\n",
    "            \"source\": r[\"source\"],\n",
    "            \"chunk_id\": int(r[\"chunk_id\"]),\n",
    "            \"algo\": \"keyword\"\n",
    "        } for r in rows]\n",
    "        con.close()\n",
    "        return out\n",
    "    finally:\n",
    "        try: con.close()\n",
    "        except: pass\n",
    "\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        bm25 = float(r[\"bm25\"]) if r[\"bm25\"] is not None else 0.0\n",
    "        out.append({\n",
    "            \"score\": -bm25,               # invert so higher is better\n",
    "            \"kw_bm25\": bm25,\n",
    "            \"text\": r[\"text\"],\n",
    "            \"source\": r[\"source\"],\n",
    "            \"chunk_id\": int(r[\"chunk_id\"]),\n",
    "            \"algo\": \"keyword\"\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ----------------------------- MMR (diversified dense) ------------------------------\n",
    "def search_mmr(\n",
    "    query: str,\n",
    "    k: int = 5,\n",
    "    fetch_k: int = 40,        # how many dense candidates to fetch before diversification\n",
    "    lambda_mult: float = 0.6, # 1.0â†’pure relevance, 0.0â†’pure diversity\n",
    "    per_source_cap: int = 1   # at most N chunks from the same PDF\n",
    "):\n",
    "    \"\"\"MMR over dense candidates to promote diversity and reduce same-doc duplicates.\"\"\"\n",
    "    if 'embed_texts' not in globals():\n",
    "        raise RuntimeError(\"embed_texts() is not defined.\")\n",
    "    if 'index' not in globals():\n",
    "        raise RuntimeError(\"Vector index not found.\")\n",
    "\n",
    "    # Encode query (embeddings normalized)\n",
    "    q = embed_texts([query])[0]\n",
    "    n = min(fetch_k, len(docs))\n",
    "    if n <= 0:\n",
    "        return []\n",
    "\n",
    "    D, I = index.search(q[None, :], n)\n",
    "    cand_ids = I[0].tolist()\n",
    "    if not cand_ids:\n",
    "        return []\n",
    "\n",
    "    cand_embs = embs[cand_ids]      # (n, d)\n",
    "    sim_q = cand_embs @ q           # (n,)\n",
    "\n",
    "    selected = []\n",
    "    used_per_src = defaultdict(int)\n",
    "    masked = np.zeros(len(cand_ids), dtype=bool)\n",
    "\n",
    "    while len(selected) < k and not masked.all():\n",
    "        if selected:\n",
    "            S = cand_embs[selected]                       # (m, d)\n",
    "            penalty = (cand_embs @ S.T).max(axis=1)      # (n,)\n",
    "        else:\n",
    "            penalty = np.zeros(len(cand_ids), dtype=\"float32\")\n",
    "\n",
    "        mmr = lambda_mult * sim_q - (1.0 - lambda_mult) * penalty\n",
    "\n",
    "        # Mask already-chosen and sources exceeding the cap\n",
    "        for j in range(len(cand_ids)):\n",
    "            if masked[j]:\n",
    "                mmr[j] = -1e9\n",
    "                continue\n",
    "            src = metadatas[cand_ids[j]][\"source\"]\n",
    "            if used_per_src[src] >= per_source_cap:\n",
    "                mmr[j] = -1e9\n",
    "\n",
    "        j_best = int(np.argmax(mmr))\n",
    "        if mmr[j_best] <= -1e8:\n",
    "            break\n",
    "\n",
    "        selected.append(j_best)\n",
    "        masked[j_best] = True\n",
    "        src = metadatas[cand_ids[j_best]][\"source\"]\n",
    "        used_per_src[src] += 1\n",
    "\n",
    "    results = []\n",
    "    for j in selected:\n",
    "        idx = cand_ids[j]\n",
    "        results.append({\n",
    "            \"score\": float(sim_q[j]),\n",
    "            \"text\": docs[idx],\n",
    "            \"source\": metadatas[idx][\"source\"],\n",
    "            \"chunk_id\": int(idx),\n",
    "            \"algo\": \"mmr\"\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ----------------------------- Hybrid (dense + keyword) -----------------------------\n",
    "def _minmax(arr):\n",
    "    a = np.asarray(arr, dtype=\"float32\")\n",
    "    lo, hi = float(np.min(a)), float(np.max(a))\n",
    "    if hi - lo < 1e-9:\n",
    "        return np.ones_like(a) * 0.5\n",
    "    return (a - lo) / (hi - lo)\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    k: int = 3,\n",
    "    k_dense: int = 10,\n",
    "    k_kw: int = 10,\n",
    "    alpha: float = 0.6,    # weight on dense (0..1)\n",
    "    use_mmr: bool = False  # set True to use diversified dense candidates\n",
    "):\n",
    "    \"\"\"Blend normalized dense and keyword scores into a single ranking.\"\"\"\n",
    "    d_hits = search_mmr(query, k=k_dense, fetch_k=max(40, 3*k_dense), lambda_mult=0.6, per_source_cap=1) \\\n",
    "             if use_mmr else dense_search(query, k=k_dense)\n",
    "    kw_hits = keyword_search(query, k=k_kw)\n",
    "\n",
    "    by_id = {}\n",
    "    for h in d_hits:\n",
    "        by_id.setdefault(h[\"chunk_id\"], {\"text\": h[\"text\"], \"source\": h[\"source\"]})\n",
    "        by_id[h[\"chunk_id\"]][\"dense\"] = h[\"score\"]\n",
    "\n",
    "    for h in kw_hits:\n",
    "        by_id.setdefault(h[\"chunk_id\"], {\"text\": h[\"text\"], \"source\": h[\"source\"]})\n",
    "        by_id[h[\"chunk_id\"]][\"kw_raw\"] = h.get(\"kw_bm25\", -h[\"score\"])  # if we inverted earlier\n",
    "\n",
    "    ids = list(by_id.keys())\n",
    "    dense_scores = [by_id[i].get(\"dense\", 0.0) for i in ids]\n",
    "\n",
    "    # For keyword: smaller bm25 is better â†’ invert (negate) then normalize\n",
    "    kw_raw = [by_id[i].get(\"kw_raw\", None) for i in ids]\n",
    "    max_bad = max([x for x in kw_raw if x is not None], default=1.0)\n",
    "    kw_inv = [-(x if x is not None else max_bad*1.2) for x in kw_raw]\n",
    "\n",
    "    dense_n = _minmax(dense_scores)\n",
    "    kw_n    = _minmax(kw_inv)\n",
    "    final   = alpha * dense_n + (1.0 - alpha) * kw_n\n",
    "\n",
    "    merged = [{\n",
    "        \"chunk_id\": i,\n",
    "        \"source\": by_id[i][\"source\"],\n",
    "        \"text\":   by_id[i][\"text\"],\n",
    "        \"dense\":  float(dense_n[j]),\n",
    "        \"kw\":     float(kw_n[j]),\n",
    "        \"score\":  float(final[j]),     # higher is better\n",
    "        \"algo\":   \"hybrid\"\n",
    "    } for j, i in enumerate(ids)]\n",
    "\n",
    "    merged.sort(key=lambda r: r[\"score\"], reverse=True)\n",
    "    return merged[:k]\n",
    "\n",
    "# ----------------------------- Quick smoke tests (optional) -------------------------\n",
    "# print(dense_search(\"what problems are studied?\", k=3))\n",
    "# print(keyword_search(\"dataset OR benchmark\", k=3))\n",
    "# print(search_mmr(\"limitations\", k=3))\n",
    "# print(hybrid_search(\"evaluation metrics\", k=3, use_mmr=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574e44f",
   "metadata": {},
   "source": [
    "## Evaluation cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d432e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, re\n",
    "from pathlib import Path\n",
    "\n",
    "DB_PATH = Path(INDEX_DIR) / \"rag.db\" if 'INDEX_DIR' in globals() else Path(\"artifacts/rag.db\")\n",
    "\n",
    "def _fts_query_from_text(text: str) -> str:\n",
    "    # keep only word tokens; quote each to avoid FTS operators/punct errors\n",
    "    toks = re.findall(r\"[A-Za-z0-9_]+\", text.lower())\n",
    "    # AND semantics by space-joining quoted tokens; works with tokenize='porter'\n",
    "    return \" \".join(f'\"{t}\"' for t in toks) if toks else \"\"\n",
    "\n",
    "def keyword_search(query: str, k: int = 5):\n",
    "    if not DB_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Keyword index not found at {DB_PATH}. Build the SQLite FTS DB first.\")\n",
    "\n",
    "    q_fts = _fts_query_from_text(query)\n",
    "    if not q_fts:\n",
    "        return []\n",
    "\n",
    "    con = sqlite3.connect(DB_PATH)\n",
    "    con.row_factory = sqlite3.Row\n",
    "    try:\n",
    "        # bm25() may not exist on very old SQLite builds; we catch that below\n",
    "        rows = con.execute(\"\"\"\n",
    "            SELECT c.rowid AS chunk_id, cm.source, c.text, bm25(chunks_fts) AS bm25\n",
    "            FROM chunks_fts c\n",
    "            JOIN chunk_meta cm ON cm.chunk_id = c.rowid\n",
    "            WHERE chunks_fts MATCH ?\n",
    "            ORDER BY bm25 ASC\n",
    "            LIMIT ?\n",
    "        \"\"\", (q_fts, k)).fetchall()\n",
    "        out = [{\n",
    "            \"score\": -float(r[\"bm25\"]),     # invert so higher=better\n",
    "            \"kw_bm25\": float(r[\"bm25\"]),\n",
    "            \"text\": r[\"text\"],\n",
    "            \"source\": r[\"source\"],\n",
    "            \"chunk_id\": int(r[\"chunk_id\"]),\n",
    "            \"algo\": \"keyword\"\n",
    "        } for r in rows]\n",
    "    except sqlite3.OperationalError:\n",
    "        # Fallback when bm25() or fts5 module lacks bm25 ranking\n",
    "        rows = con.execute(\"\"\"\n",
    "            SELECT c.rowid AS chunk_id, cm.source, c.text\n",
    "            FROM chunks_fts c\n",
    "            JOIN chunk_meta cm ON cm.chunk_id = c.rowid\n",
    "            WHERE chunks_fts MATCH ?\n",
    "            LIMIT ?\n",
    "        \"\"\", (q_fts, k)).fetchall()\n",
    "        out = [{\n",
    "            \"score\": 0.0,\n",
    "            \"kw_bm25\": None,\n",
    "            \"text\": r[\"text\"],\n",
    "            \"source\": r[\"source\"],\n",
    "            \"chunk_id\": int(r[\"chunk_id\"]),\n",
    "            \"algo\": \"keyword\"\n",
    "        } for r in rows]\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0577a1d",
   "metadata": {},
   "source": [
    "## Retrieval Report\n",
    "Generate a markdown report with â‰¥5 queries and their top-3 retrieved passages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e98d95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, datetime\n",
    "\n",
    "def run_retrieval_report(queries, k=3, out_path=os.path.join(INDEX_DIR, \"retrieval_report.md\")):\n",
    "    \"\"\"\n",
    "    Generates a markdown report with >=5 queries and their top-k retrieved passages.\n",
    "    \"\"\"\n",
    "    ts = datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "    lines = [f\"# Retrieval Report\\n\\nGenerated: {ts}\\n\\n\"]\n",
    "    for qi, q in enumerate(queries, 1):\n",
    "        lines.append(f\"## Q{qi}. {q}\\n\")\n",
    "        hits = search(q, k=k)\n",
    "        for i, h in enumerate(hits, 1):\n",
    "            excerpt = h[\"text\"][:800].replace(\"\\n\", \" \")\n",
    "            lines.append(f\"**Top {i}** (score {h['score']:.3f}) â€” *{h['source']}*\\n\\n> {excerpt} â€¦\\n\")\n",
    "        lines.append(\"\\n---\\n\")\n",
    "    pathlib.Path(out_path).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\"Saved report to {out_path}\")\n",
    "\n",
    "# Example starter queries (customize for your paper set)\n",
    "example_queries = [\n",
    "    \"What problem do these papers address?\",\n",
    "    \"Summarize the main methodology used.\",\n",
    "    \"What datasets are commonly used?\",\n",
    "    \"What are the key contributions mentioned?\",\n",
    "    \"What future work or limitations are discussed?\"\n",
    "]\n",
    "\n",
    "# Uncomment to run:\n",
    "# run_retrieval_report(example_queries, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c00d4",
   "metadata": {},
   "source": [
    "## Simple RAG answer (LLM call placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fa9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda2072b364e40deac91d2665d4c6207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What problem do these papers most commonly address?\n",
      "--------------------------------------------------------------------------------\n",
      "The papers most commonly address issues related to the evaluation and understanding of complex systems, particularly in the fields of medical imaging and artificial intelligence. \n",
      "\n",
      "The first paper focuses on the development of a dataset for visual question answering in gastrointestinal imaging, emphasizing the complexity of questions and the clinical categorization of findings in medical images [1]. \n",
      "\n",
      "The second paper discusses the challenges of detecting harmful content online and the implications of misinformation, highlighting the need for better understanding and evaluation of AI-generated content [2]. \n",
      "\n",
      "The third paper examines the inconsistencies in explanation methods for AI models, proposing a probabilistic and spectral analysis to better understand these issues and suggesting preliminary solutions [3]. \n",
      "\n",
      "Overall, these works collectively address the complexities and challenges in interpreting and evaluating AI systems and their outputs in various contexts.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10869v1 - Medico 2025 Visual Question Answering for Gastrointestinal Imaging.pdf  (score=0.800, dense=1.000, kw=0.500)\n",
      "[2] 2508.10769v1 - Modeling Human Responses to Multimodal AI Content.pdf  (score=0.667, dense=0.779, kw=0.500)\n",
      "[3] 2508.10595v1 - On Spectral Properties of Gradient-based Explanation Methods.pdf  (score=0.470, dense=0.450, kw=0.500)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b526954e210a4eacbac2221dc5ee74af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: Summarize the typical methodology or architecture described across the papers.\n",
      "--------------------------------------------------------------------------------\n",
      "The typical methodology described in the papers involves a structured approach to design evaluation and AI system functionality. \n",
      "\n",
      "In the **Agentic Design Review System** outlined in [1], the methodology includes a **Structured Design Description (SDD)** module that generates design descriptions to guide the review process. This system employs a **meta agent** to coordinate both static and dynamic agents. Static agents evaluate fixed design attributes, while dynamic agents assess attributes contextualized to specific designs. The meta agent consolidates insights from these evaluations to provide a final rating and actionable feedback, fostering a structured critique process that balances subjective creativity with objective design principles.\n",
      "\n",
      "In the context of AI systems, as discussed in [2], various methodologies are categorized based on their functionality, such as **descriptive**, **predictive**, **prescriptive**, and **generative** approaches. Each category serves distinct purposes, from analyzing historical data to generating new content based on learned patterns. This classification highlights the diverse applications of AI in tasks ranging from sales forecasting to personalized marketing.\n",
      "\n",
      "Overall, the methodologies emphasize collaborative evaluation and the use of structured frameworks to enhance decision-making and feedback processes in design and AI applications.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10745v1 - Agentic Design Review System.pdf  (score=0.800, dense=1.000, kw=0.500)\n",
      "[2] 2508.10806v1 - Who Benefits from AI Explanations Towards Accessible and Interpretable Systems.pdf  (score=0.440, dense=0.399, kw=0.500)\n",
      "[3] 2508.10667v1 - AddressVLM Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models.pdf  (score=0.333, dense=0.222, kw=0.500)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ff2736dbe54bc3a3052685759feb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: Which datasets or benchmarks are most frequently mentioned?\n",
      "--------------------------------------------------------------------------------\n",
      "The most frequently mentioned datasets or benchmarks in the provided context are:\n",
      "\n",
      "1. **Landmark-4K dataset** - A proposed dataset consisting of 185 high-quality landmark images covering 49 categories, used for evaluating image super-resolution methods [1].\n",
      "2. **CUFED5 dataset** - A testing dataset consisting of 126 image pairs, utilized in the evaluation of super-resolution methods [1].\n",
      "3. **WR-SR dataset** - A dataset with 80 images, also used for evaluating super-resolution techniques [1].\n",
      "4. **UniBench300** - A unified benchmark that incorporates multi-task data for visual object tracking and benchmarking [2].\n",
      "5. **LasHeR, VisEvent, and DepthTrack** - Datasets used for training and evaluation in the context of unified tracking methods [2].\n",
      "6. **CIFAR-100 and ImageNet-R** - Datasets used for efficiency evaluations in federated learning [3].\n",
      "\n",
      "These datasets serve various purposes in the evaluation of image super-resolution and visual object tracking methods.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10779v1 - Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior.pdf  (score=0.800, dense=1.000, kw=0.500)\n",
      "[2] 2508.10655v1 - Serial Over Parallel Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking.pdf  (score=0.729, dense=0.882, kw=0.500)\n",
      "[3] 2508.10732v1 - APFL Analytic Personalized Federated Learning via Dual-Stream Least Squares.pdf  (score=0.629, dense=0.715, kw=0.500)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e2d7b9a7f469ea8b0db8f5d43f292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What key limitations or open challenges are discussed?\n",
      "--------------------------------------------------------------------------------\n",
      "The key limitations or open challenges discussed include:\n",
      "\n",
      "1. The generalization gap across different skills in reasoning tasks, where advanced reasoning capabilities are a significant differentiator among models, particularly in complex reasoning tasks compared to more direct perceptual tasks [3].\n",
      "2. The brittleness of instruction-following capabilities in models, which can be context-dependent and lead to failures in specific tasks despite strong performance in others [3].\n",
      "\n",
      "These challenges highlight the need for improvements in reasoning and instruction adherence in large language models.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10751v1 - Passk Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models.pdf  (score=0.800, dense=1.000, kw=0.500)\n",
      "[2] 2508.10599v1 - MSRS Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models.pdf  (score=0.535, dense=0.558, kw=0.500)\n",
      "[3] 2508.10729v1 - EgoCross Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering.pdf  (score=0.457, dense=0.428, kw=0.500)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a510a2a96b1d490ca99980319bf7d5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What future directions or proposed improvements recur across the papers?\n",
      "--------------------------------------------------------------------------------\n",
      "The future directions and proposed improvements across the papers include:\n",
      "\n",
      "1. **Adaptive Unmasking/Remasking Strategies**: The use of adaptive strategies like remasking in diffusion language models is highlighted as a way to enhance efficiency and quality. This approach allows for a trade-off between compute resources and output quality, which is essential for improving model performance during inference [1].\n",
      "\n",
      "2. **Guidance Techniques**: The implementation of guidance techniques, such as classifier-free guidance, is emphasized as a method to improve the quality of generated outputs by steering the generative process towards desired attributes. This technique has become foundational in various text-to-image systems and is being adopted for prompt-controlled generation in diffusion language models [1].\n",
      "\n",
      "3. **Transparency and Explainability in AI**: There is a growing need for regulations and legislation that mandate transparency and explainability in AI systems. The papers suggest that addressing these issues is crucial for developing ethical and fair AI practices, as current efforts are hampered by trade secret concerns and resistance from some governments [3].\n",
      "\n",
      "These themes indicate a focus on enhancing model efficiency, output quality, and ethical considerations in AI development.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10875v1 - A Survey on Diffusion Language Models.pdf  (score=0.800, dense=1.000, kw=0.500)\n",
      "[2] 2508.10667v1 - AddressVLM Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models.pdf  (score=0.607, dense=0.678, kw=0.500)\n",
      "[3] 2508.10806v1 - Who Benefits from AI Explanations Towards Accessible and Interpretable Systems.pdf  (score=0.536, dense=0.560, kw=0.500)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, textwrap\n",
    "from typing import List, Dict, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def build_prompt(question: str, passages):\n",
    "    lines = [\n",
    "        \"You are a careful assistant. Answer ONLY using the context below.\",\n",
    "        \"Cite sources like [1], [2]. If at least two sources are provided, use at least two distinct citations.\",\n",
    "        \"If the answer is not covered, say you don't have enough information.\",\n",
    "        \"\",\n",
    "        f\"Question: {question}\",\n",
    "        \"\",\n",
    "        \"Context:\",\n",
    "    ]\n",
    "    for i, p in enumerate(passages, 1):\n",
    "        snippet = p[\"text\"].strip()[:1800]\n",
    "        lines.append(f\"[{i}] ({p['source']})\\n{snippet}\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _get_openai_client() -> OpenAI:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\n",
    "            \"OPENAI_API_KEY is not set. Run the setup cell to set it, \"\n",
    "            \"or create the client with OpenAI(api_key='...').\"\n",
    "        )\n",
    "    return OpenAI(api_key=api_key)\n",
    "\n",
    "def ask_llm(\n",
    "    question: str,\n",
    "    k: int = 3,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    alpha: float = 0.6,       # weight for dense vs keyword in hybrid\n",
    "    use_mmr: bool = True      # diversify dense candidates before merging\n",
    ") -> Dict:\n",
    "    # Retrieve (hybrid if available; else fall back to dense 'search')\n",
    "    try:\n",
    "        hits = hybrid_search(\n",
    "            question,\n",
    "            k=k,\n",
    "            k_dense=10,\n",
    "            k_kw=10,\n",
    "            alpha=alpha,\n",
    "            use_mmr=use_mmr\n",
    "        )\n",
    "    except NameError:\n",
    "        hits = search(question, k=k)  # fallback to your original dense search\n",
    "\n",
    "    if not hits:\n",
    "        return {\"question\": question, \"answer\": \"(no passages retrieved)\", \"hits\": []}\n",
    "\n",
    "    # Prompt\n",
    "    prompt = build_prompt(question, hits)\n",
    "\n",
    "    # Call OpenAI\n",
    "    client = _get_openai_client()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    answer = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"-\"*80)\n",
    "    print(answer)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Sources:\")\n",
    "    for i, h in enumerate(hits, 1):\n",
    "        dense = f\"{h.get('dense', float('nan')):.3f}\" if 'dense' in h else \"-\"\n",
    "        kw = f\"{h.get('kw', float('nan')):.3f}\"       if 'kw' in h else \"-\"\n",
    "        print(f\"[{i}] {h['source']}  (score={h['score']:.3f}, dense={dense}, kw={kw})\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    return {\"question\": question, \"answer\": answer, \"hits\": hits}\n",
    "\n",
    "# ---- Ask 5 relevant questions about your corpus ----\n",
    "five_questions = [\n",
    "    \"What problem do these papers most commonly address?\",\n",
    "    \"Summarize the typical methodology or architecture described across the papers.\",\n",
    "    \"Which datasets or benchmarks are most frequently mentioned?\",\n",
    "    \"What key limitations or open challenges are discussed?\",\n",
    "    \"What future directions or proposed improvements recur across the papers?\"\n",
    "]\n",
    "\n",
    "results = [ask_llm(q, k=3) for q in five_questions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b535a4",
   "metadata": {},
   "source": [
    "## FastAPI endpoints (default top-3)\n",
    "Run with: `uvicorn app:app --reload --port 8000`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e572ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query: str\n",
    "    k: int = 3\n",
    "    alpha: float = 0.6\n",
    "\n",
    "@app.post(\"/hybrid_search\")\n",
    "def do_hybrid(q: Query):\n",
    "    hits = hybrid_search(q.query, k=q.k, k_dense=10, k_kw=10, alpha=q.alpha)\n",
    "    for h in hits: h[\"text\"] = h[\"text\"][:500]\n",
    "    return {\"results\": hits}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
