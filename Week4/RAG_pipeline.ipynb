{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install --upgrade --prefer-binary sentence-transformers faiss-cpu pymupdf fastapi uvicorn pydantic numpy tiktoken\n",
    "\n",
    "# Sanity check: make sure the kernel sees faiss\n",
    "import sys, platform, pkgutil\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Kernel executable:\", sys.executable)\n",
    "print(\"faiss importable? ->\", pkgutil.find_loader(\"faiss\") is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93386c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu pymupdf fastapi uvicorn pydantic numpy tiktoken hf_xet\n",
    "\n",
    "import os, json, re\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "\n",
    "DATA_DIR = \"data/arxiv\"\n",
    "INDEX_DIR = \"artifacts\"  \n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60e430",
   "metadata": {},
   "source": [
    "## Loading PDFs and extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b298698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 PDFs: ['2508.10507v1 - Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting.pdf', '2508.10528v1 - Med-GLIP Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset.pdf', '2508.10530v1 - Diversity First Quality Later A Two-Stage Assumption for Language Model Alignment.pdf', '2508.10539v1 - Improving Value-based Process Verifier via Low-Cost Variance Reduction.pdf', '2508.10548v1 - Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards.pdf'] ...\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    doc = fitz.open(path)\n",
    "    pages = []\n",
    "    for p in doc:\n",
    "        pages.append(p.get_text(\"text\"))\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "def load_corpus(pdf_dir: str = DATA_DIR) -> Dict[str, str]:\n",
    "    corpus = {}\n",
    "    for name in os.listdir(pdf_dir):\n",
    "        if name.lower().endswith(\".pdf\"):\n",
    "            fp = os.path.join(pdf_dir, name)\n",
    "            corpus[name] = extract_text_from_pdf(fp)\n",
    "    return corpus\n",
    "\n",
    "corpus = load_corpus()\n",
    "print(f\"Loaded {len(corpus)} PDFs: {list(corpus.keys())[:5]}{' ...' if len(corpus)>5 else ''}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d631e",
   "metadata": {},
   "source": [
    "## Chunking (token-based, ~512 tokens with overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c87143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2055 from 50 PDFs\n"
     ]
    }
   ],
   "source": [
    "# Token-based chunking (~512 tokens) with fallback to character-based if tiktoken isn't available\n",
    "from typing import List\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    _enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "except Exception:\n",
    "    _enc = None\n",
    "\n",
    "def chunk_by_tokens(text: str, max_tokens: int = 512, overlap_tokens: int = 64) -> List[str]:\n",
    "    \"\"\"\n",
    "    Token-based sliding window chunker targeting ~512 tokens per chunk with ~64-token overlap.\n",
    "    Falls back to character-based (~4 chars/token heuristic) if tiktoken isn't available.\n",
    "    \"\"\"\n",
    "    if _enc is None:\n",
    "        approx = max_tokens * 4\n",
    "        overlap_chars = overlap_tokens * 4\n",
    "        clean = \" \".join(text.split())\n",
    "        chunks, start = [], 0\n",
    "        while start < len(clean):\n",
    "            end = min(start + approx, len(clean))\n",
    "            chunks.append(clean[start:end])\n",
    "            if end == len(clean): break\n",
    "            start = end - overlap_chars\n",
    "        return chunks\n",
    "\n",
    "    tokens = _enc.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        sub = tokens[start:end]\n",
    "        chunks.append(_enc.decode(sub))\n",
    "        if end == len(tokens): break\n",
    "        start = end - overlap_tokens\n",
    "    return chunks\n",
    "\n",
    "# Build chunk list with metadata using token-based chunking\n",
    "docs: List[str] = []\n",
    "metadatas: List[Dict] = []\n",
    "\n",
    "for fname, text in corpus.items():\n",
    "    for ch in chunk_by_tokens(text, max_tokens=512, overlap_tokens=64):\n",
    "        docs.append(ch)\n",
    "        metadatas.append({\"source\": fname})\n",
    "\n",
    "print(f\"Total chunks: {len(docs)} from {len(corpus)} PDFs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112c477",
   "metadata": {},
   "source": [
    "## Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a24745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jywu7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 65/65 [00:46<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2055, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    # normalize_embeddings=True pairs well with FAISS inner-product search\n",
    "    embs = model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "    return np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "embs = embed_texts(docs)\n",
    "embs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97d5c5",
   "metadata": {},
   "source": [
    "## Build FAISS index & persist artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4afadb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index to artifacts\\arxiv.index and chunks to artifacts\\chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "if len(docs) == 0:\n",
    "    raise RuntimeError(\"No chunks found. Add PDFs to data/arxiv/ and rerun from the top.\")\n",
    "\n",
    "dim = embs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # inner product on normalized vectors ≈ cosine similarity\n",
    "index.add(embs)\n",
    "\n",
    "# Persist index\n",
    "faiss_path = os.path.join(INDEX_DIR, \"arxiv.index\")\n",
    "faiss.write_index(index, faiss_path)\n",
    "\n",
    "# Persist chunks (text + source) as JSONL so you can reload later\n",
    "chunks_path = os.path.join(INDEX_DIR, \"chunks.jsonl\")\n",
    "with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for t, m in zip(docs, metadatas):\n",
    "        f.write(json.dumps({\"text\": t, \"source\": m[\"source\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved index to {faiss_path} and chunks to {chunks_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4c312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_and_chunks(index_dir: str = INDEX_DIR) -> Tuple[faiss.Index, List[str], List[Dict]]:\n",
    "    idx = faiss.read_index(os.path.join(index_dir, \"arxiv.index\"))\n",
    "    dcs, metas = [], []\n",
    "    with open(os.path.join(index_dir, \"chunks.jsonl\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            dcs.append(rec[\"text\"])\n",
    "            metas.append({\"source\": rec[\"source\"]})\n",
    "    return idx, dcs, metas\n",
    "\n",
    "# Example usage (uncomment to test reloading):\n",
    "# index, docs, metadatas = load_index_and_chunks()\n",
    "# print(len(docs), \"chunks reloaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c28bcb",
   "metadata": {},
   "source": [
    "## Search helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf68cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.30652010440826416,\n",
       "  'text': \" located? Answer the \\nquestion using a single phrase.\\n1. Unidentified residential street. [Failed]\\n2. residential street in an unnamed district.\\n[Failed]\\n1. I'm unable to determine the specific street \\nand district based on the image alone. [Failed]\\n2. I cannot determine the specific … [Failed]\\nFreyburg Street, South Side Flats\\n1. Hill District of Pittsburgh, Pennsylvania.\\n2. The image is located in a residential \\ndistrict, but the specific street … [Failed]\\n1. Santa fe, downtown [Hallucination]\\n2. Rocky hill [Invalid]\\n3. 1st street [Hallucination]\\nGPT-4o\\nQwen2-VL\\nAddressVLM\\nSonnet\\nLLaVA\\nFig. 10 Qualitative comparison of address question-answering capabilities with general LVLMs.\\n17\\n\\n5 Conclusion\\nIn this work, we propose AddressVLM for city-wide address localization, which can\\nperform flexible address question-answering for street-view images. The core idea is\\nto leverage cross-view alignment tuning between satellite-view images and street-view\\nimages to integrate a global understanding of street distribution into LVLM. This\\ncontains two key components, namely the satellite and street view image grafting\\nmechanism, and the automatic alignment label generation mechanism. The model\\nundergoes two-stage fine-tuning, including cross-view alignment tuning and address\\nlocalization tuning. Extensive experiments show that the proposed AddressVLM sur-\\npasses general LVLMs and SOTA localization LVLMs, and can be extended to multiple\\ncities. In future work, we would like to explore cities on different continents and adopt\\nlarger LVLMs.\\nLimitations. Thanks to the proposed image crafting mechanism, AddressVLM can\\nperform cross-view alignment tuning. The current method is only a preliminary explo-\\nration, the relatively low resolution of street view images may affect the LVLM’s ability\\nto understand them. In the future, more sophisticated cross-view image alignment\\nmethods are worth studying to further improve performance.\\nAppendix A\\nImplementation Details\\nAll our experiments are conducted using the xtuner framework on 8 RTX 3090 GPUs.\\nThe torch version is 2.4.0, the CUDA version is 12.1, and the transformers version is\\n4.37.2. The main hyperparameter settings are given in Tab. A1.\\nTable A1 Hyper-parameter settings of the both two tuning stage\",\n",
       "  'source': '2508.10667v1 - AddressVLM Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models.pdf',\n",
       "  'chunk_id': 855},\n",
       " {'score': 0.2735181152820587,\n",
       "  'text': 'ixin Liu, Shiyuan Li, Qingfeng Chen, Yu Zheng, and Shirui\\nPan. 2025. FreeGAD: A Training-Free yet Effective Approach for Graph\\nAnomaly Detection. In Proceedings of the 34th ACM International Conference\\n∗All authors contributed equally to this research.\\n†Corresponding author.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nCIKM ’25, Seoul, Republic of Korea.\\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-2040-6/2025/11\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\non Information and Knowledge Management (CIKM ’25), November 10–14,\\n2025, Seoul, Republic of Korea. ACM, New York, NY, USA, 11 pages. https:\\n//doi.org/10.1145/nnnnnnn.nnnnnnn\\n1\\nIntroduction\\nGraph anomaly detection (GAD) is a critical research area focused\\non identifying abnormal nodes that significantly deviate from nor-\\nmal patterns within a graph [27, 37]. GAD has been applied to\\nnumerous real-world scenarios, such as fraud detection in financial\\nnetworks, intrusion detection in cybersecurity, and identifying ir-\\nregularities in social or communication networks [13, 35, 51]. The\\ngrowing importance of GAD in real-world applications has led to\\nincreasing research attention being devoted to developing effective\\nand robust methods for detecting anomalies in graph-structured\\ndata [7, 25, 36].\\nDue to the difficulty of obtaining labeled anomaly data in real-\\nworld scenarios, the mainstream GAD methods primarily focus\\non unsupervised scenarios [2, 8, 36, 37]. While the early shallow\\nGAD methods often face limitations in handling complex and high-\\ndimension',\n",
       "  'source': '2508.10594v1 - FreeGAD A Training-Free yet Effective Approach for Graph Anomaly Detection.pdf',\n",
       "  'chunk_id': 311},\n",
       " {'score': 0.2656620144844055,\n",
       "  'text': \"o\\nGPT-4o\\nQwen2-VL\\nQwen2-VL\\nQwen2-VL\\nAddressVLM\\nAddressVLM\\nAddressVLM\\nSonnet\\nSonnet\\nSonnet\\nLLaVA\\nLLaVA\\nLLaVA\\nLiberty Avenue, Downtown\\nSmithfield Street, Downtown\\nLiberty Bridge, South Side Flats\\nWhich street and district is the \\nimage located? Answer the \\nquestion using a single phrase.\\nWhat district is this photo \\ntaken in? Answer the question \\nusing a single word or phrase.\\nWhat street is this photo taken \\non? Answer the question using \\na single word or phrase.\\nSmithfield Street.\\n1. I am unable to determine the street from \\nthis image alone. [Failed]\\n2. I cannot determine the street … [Failed]\\n1. City street [Hallucination]\\n2. Downtown. [District level]\\n3.The image does not provide enough … [Failed]\\n1. Philadelphia. [City level]\\n2. Downtown.\\nDowntown.\\n1. Financial District [Hallucination]\\n1. The image does not provide enough \\ncontextual information … [Failed]\\n1. Fort Pitt Boulevard, Downtown Pittsburgh.\\n2. Grant Street, Downtown Pittsburgh.\\nLiberty Bridge, South Side Flats\\n1. downtown.\\n2. Without specific landmarks or more \\ncontext, it's difficult to pinpoint … [Failed]\\n1. main. [Hallucination]\\n2. The photo is taken on Wall Street. \\n1. The image is located in the downtown \\narea of Pittsburgh, Pennsylvania.\\n2. Without specific landmarks … [Failed]\\n1. Financial [Invalid]\\n2. Downtown\\n1. Fifth avenue\\n2. Cross street [Hallucination]\\n3. Main [Hallucination]\\n1. Downtown\\n2. Chicago [City level]\\n3. 13th street [Hallucination]\\nGPT-4o\\nGPT-4o\\nGPT-4o\\nQwen2-VL\\nQwen2-VL\\nQwen2-VL\\nAddressVLM\\nAddressVLM\\nAddressVLM\\nSonnet\\nSonnet\\nSonnet\\nLLaVA\\nLLaVA\\nLLaVA\\nWashington Place, Downtown\\nMiltenberger Street, Uptown\\nFrench Street, Downtown\\nWhich street and district is the \\nimage located? Answer the \\nquestion using a single phrase.\\nWhat district is this photo \\ntaken in? Answer the question \\nusing a single word\",\n",
       "  'source': '2508.10667v1 - AddressVLM Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models.pdf',\n",
       "  'chunk_id': 853}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(query: str, k: int = 5):\n",
    "    q = embed_texts([query])  # uses the same normalization as corpus embeddings\n",
    "    D, I = index.search(q, k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        results.append({\n",
    "            \"score\": float(score),\n",
    "            \"text\": docs[idx],\n",
    "            \"source\": metadatas[idx][\"source\"],\n",
    "            \"chunk_id\": int(idx)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Smoke test\n",
    "search(\"What problem does the paper address?\", k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0577a1d",
   "metadata": {},
   "source": [
    "## Retrieval Report\n",
    "Generate a markdown report with ≥5 queries and their top-3 retrieved passages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98d95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, datetime\n",
    "\n",
    "def run_retrieval_report(queries, k=3, out_path=os.path.join(INDEX_DIR, \"retrieval_report.md\")):\n",
    "    \"\"\"\n",
    "    Generates a markdown report with >=5 queries and their top-k retrieved passages.\n",
    "    \"\"\"\n",
    "    ts = datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "    lines = [f\"# Retrieval Report\\n\\nGenerated: {ts}\\n\\n\"]\n",
    "    for qi, q in enumerate(queries, 1):\n",
    "        lines.append(f\"## Q{qi}. {q}\\n\")\n",
    "        hits = search(q, k=k)\n",
    "        for i, h in enumerate(hits, 1):\n",
    "            excerpt = h[\"text\"][:800].replace(\"\\n\", \" \")\n",
    "            lines.append(f\"**Top {i}** (score {h['score']:.3f}) — *{h['source']}*\\n\\n> {excerpt} …\\n\")\n",
    "        lines.append(\"\\n---\\n\")\n",
    "    pathlib.Path(out_path).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\"Saved report to {out_path}\")\n",
    "\n",
    "# Example starter queries (customize for your paper set)\n",
    "example_queries = [\n",
    "    \"What problem do these papers address?\",\n",
    "    \"Summarize the main methodology used.\",\n",
    "    \"What datasets are commonly used?\",\n",
    "    \"What are the key contributions mentioned?\",\n",
    "    \"What future work or limitations are discussed?\"\n",
    "]\n",
    "\n",
    "# Uncomment to run:\n",
    "# run_retrieval_report(example_queries, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c00d4",
   "metadata": {},
   "source": [
    "## LLM call + sample prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fa9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What problem do these papers most commonly address?\n",
      "--------------------------------------------------------------------------------\n",
      "The papers most commonly address the challenges related to visual question answering in the context of gastrointestinal imaging and the evaluation of misinformation on online platforms. Specifically, the first paper focuses on creating a dataset for GI endoscopic images paired with complex question-answer pairs to enhance reasoning capabilities in medical imaging [1]. The second paper discusses the implications of online searches for misinformation and how they can affect the perceived veracity of such content [2].\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10869v1 - Medico 2025 Visual Question Answering for Gastrointestinal Imaging.pdf  (score=0.288)\n",
      "[2] 2508.10769v1 - Modeling Human Responses to Multimodal AI Content.pdf  (score=0.272)\n",
      "[3] 2508.10666v1 - Deep Learning in Classical and Quantum Physics.pdf  (score=0.234)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 131.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: Summarize the typical methodology or architecture described across the papers.\n",
      "--------------------------------------------------------------------------------\n",
      "The typical methodology described in the papers involves a structured approach to design evaluation and planning. \n",
      "\n",
      "In the context of the Agentic Design Review System, the methodology includes a Structured Design Description (SDD) module that generates design descriptions to guide the review process. This system employs a meta agent to coordinate both static agents, which focus on fixed design attributes (like typography), and dynamic agents, which assess attributes contextualized to specific designs. The meta agent consolidates insights from these agents to provide a final rating and actionable feedback, fostering a nuanced critique process that balances subjective creativity with objective design principles [1].\n",
      "\n",
      "Additionally, in the realm of planning, the Planning Domain Definition Language (PDDL) is utilized to express planning problems in a structured manner. This methodology separates domain knowledge from specific problem instances, allowing for the modeling of diverse planning scenarios through symbolic representations. PDDL enables planners to solve multiple instances with shared dynamics, making it suitable for generalized planning and the development of transferable policies [3].\n",
      "\n",
      "Thus, the methodologies across the papers emphasize structured frameworks for evaluation and planning, leveraging both collaborative agent systems and symbolic modeling techniques.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10745v1 - Agentic Design Review System.pdf  (score=0.419)\n",
      "[2] 2508.10666v1 - Deep Learning in Classical and Quantum Physics.pdf  (score=0.311)\n",
      "[3] 2508.10747v1 - Scaling Up without Fading Out Goal-Aware Sparse GNN for RL-based Generalized Planning.pdf  (score=0.327)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: Which datasets or benchmarks are most frequently mentioned?\n",
      "--------------------------------------------------------------------------------\n",
      "The most frequently mentioned datasets or benchmarks in the provided context are:\n",
      "\n",
      "1. **CUFED5** - A testing dataset consisting of 126 image pairs, each with 5 reference images of varying similarity levels [1].\n",
      "2. **WR-SR** - A dataset consisting of 80 images, each paired with one reference image sourced from Google Image [1].\n",
      "3. **Landmark-4K** - A proposed dataset containing 185 high-quality landmark images across 49 categories, each with a corresponding high-quality reference image [1].\n",
      "4. **T-Finance** - A large-scale dataset used for graph anomaly detection [3].\n",
      "5. **Elliptic** - Another large-scale dataset mentioned in the context of graph anomaly detection [3].\n",
      "6. **BlogCatalog** and **Flickr** - Datasets used to evaluate the performance of FreeGAD, particularly in terms of efficiency and memory usage [3].\n",
      "\n",
      "These datasets are utilized for various evaluations and comparisons in the respective studies.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10779v1 - Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior.pdf  (score=0.500)\n",
      "[2] 2508.10887v1 - Empirical Investigation into Configuring Echo State Networks for Representative Benchmark Problem Domains.pdf  (score=0.363)\n",
      "[3] 2508.10594v1 - FreeGAD A Training-Free yet Effective Approach for Graph Anomaly Detection.pdf  (score=0.454)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What key limitations or open challenges are discussed?\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not specify any key limitations or open challenges. Therefore, I don't have enough information to answer the question.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10751v1 - Passk Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models.pdf  (score=0.465)\n",
      "[2] 2508.10701v1 - REFN A Reinforcement-Learning-From-Network Framework against 1-dayn-day Exploitations.pdf  (score=0.294)\n",
      "[3] 2508.10806v1 - Who Benefits from AI Explanations Towards Accessible and Interpretable Systems.pdf  (score=0.295)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 111.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Q: What future directions or proposed improvements recur across the papers?\n",
      "--------------------------------------------------------------------------------\n",
      "The future directions and proposed improvements across the papers include:\n",
      "\n",
      "1. **Adaptive Unmasking/Remasking Strategies**: The use of adaptive unmasking and remasking strategies in diffusion language models is highlighted as a means to enhance efficiency and quality. This includes techniques like the remasking sampler proposed by ReMDM, which allows for further refinement of already decoded tokens, thus improving the compute-quality trade-off [1].\n",
      "\n",
      "2. **Guidance Techniques**: The implementation of guidance techniques, such as classifier-free guidance, is emphasized as a pivotal method for steering generative models towards desired outputs. This approach enhances the quality of generated samples by balancing fidelity to conditions against sample diversity [1].\n",
      "\n",
      "3. **Regulatory and Ethical Frameworks**: There is a pressing need for regulations and legislation that mandate transparency and explainability in AI systems. The lack of such policies can lead to negligence in developing ethical AI practices. The call for more inclusive and comprehensible AI systems is underscored, particularly in the context of accessibility to AI explanations [3].\n",
      "\n",
      "4. **Improved Vulnerability Fixing Approaches**: The REFN framework demonstrates significant improvements in vulnerability fixing compared to alternative methods, suggesting a direction towards more effective and automated solutions in cybersecurity [2].\n",
      "\n",
      "These themes reflect a collective focus on enhancing model efficiency, output quality, ethical considerations, and security measures in AI systems.\n",
      "--------------------------------------------------------------------------------\n",
      "Sources:\n",
      "[1] 2508.10875v1 - A Survey on Diffusion Language Models.pdf  (score=0.377)\n",
      "[2] 2508.10701v1 - REFN A Reinforcement-Learning-From-Network Framework against 1-dayn-day Exploitations.pdf  (score=0.287)\n",
      "[3] 2508.10806v1 - Who Benefits from AI Explanations Towards Accessible and Interpretable Systems.pdf  (score=0.325)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, textwrap\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def build_prompt(question: str, passages):\n",
    "    lines = [\n",
    "        \"You are a careful assistant. Answer ONLY using the context below.\",\n",
    "        \"Cite sources like [1], [2]. If at least two sources are provided, use at least two distinct citations.\",\n",
    "        \"If the answer is not covered, say you don't have enough information.\",\n",
    "        \"\",\n",
    "        f\"Question: {question}\",\n",
    "        \"\",\n",
    "        \"Context:\",\n",
    "    ]\n",
    "    for i, p in enumerate(passages, 1):\n",
    "        snippet = p[\"text\"].strip()[:1800]\n",
    "        lines.append(f\"[{i}] ({p['source']})\\n{snippet}\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def ask_llm(question: str, k: int = 3, model: str = \"gpt-4o-mini\") -> Dict:\n",
    "    hits = search(question, k=k)\n",
    "\n",
    "    if not hits:\n",
    "        return {\"question\": question, \"answer\": \"(no passages retrieved)\", \"hits\": []}\n",
    "\n",
    "    # Prompt\n",
    "    prompt = build_prompt(question, hits)\n",
    "\n",
    "    # Call OpenAI\n",
    "    client = OpenAI()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    answer = resp.choices[0].message.content.strip()\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"-\"*80)\n",
    "    print(answer)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Sources:\")\n",
    "    for i, h in enumerate(hits, 1):\n",
    "        print(f\"[{i}] {h['source']}  (score={h['score']:.3f})\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    return {\"question\": question, \"answer\": answer, \"hits\": hits}\n",
    "\n",
    "# ---- Ask 5 relevant questions about your corpus ----\n",
    "five_questions = [\n",
    "    \"What problem do these papers most commonly address?\",\n",
    "    \"Summarize the typical methodology or architecture described across the papers.\",\n",
    "    \"Which datasets or benchmarks are most frequently mentioned?\",\n",
    "    \"What key limitations or open challenges are discussed?\",\n",
    "    \"What future directions or proposed improvements recur across the papers?\"\n",
    "]\n",
    "\n",
    "results = [ask_llm(q, k=3) for q in five_questions]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b535a4",
   "metadata": {},
   "source": [
    "## FastAPI endpoints (default top-3)\n",
    "Run with: `uvicorn app:app --reload --port 8000`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e572ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "    k: int = 3  \n",
    "\n",
    "@app.post(\"/query\")\n",
    "def do_query(q: Query):\n",
    "    hits = search(q.question, k=q.k)\n",
    "    # You can add an LLM call here and return {\"answer\": \"...\", \"sources\": hits}\n",
    "    return {\"results\": hits}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
