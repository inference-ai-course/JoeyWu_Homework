{"video_id": "Xe3H2R_2Ta4", "url": "https://www.youtube.com/watch?v=Xe3H2R_2Ta4", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.0, "text": " Hello, I'm Emily Duhm, a final international teaching seminar.", "tokens": [50364, 2425, 11, 286, 478, 15034, 413, 3232, 76, 11, 257, 2572, 5058, 4571, 29235, 13, 50814], "temperature": 0.0, "avg_logprob": -0.32639122009277344, "compression_ratio": 1.45, "no_speech_prob": 0.07028607279062271}, {"id": 1, "seek": 0, "start": 9.0, "end": 13.0, "text": " I'm Robert Dills, founder of NLP University here in Santa Cruz, California.", "tokens": [50814, 286, 478, 7977, 413, 2565, 11, 14917, 295, 426, 45196, 3535, 510, 294, 9933, 23008, 11, 5384, 13, 51014], "temperature": 0.0, "avg_logprob": -0.32639122009277344, "compression_ratio": 1.45, "no_speech_prob": 0.07028607279062271}, {"id": 2, "seek": 0, "start": 13.0, "end": 29.0, "text": " We're also both authors of many books, and we'd like to share with you about a topic that we're going to be presenting at the conference that relates to our upcoming book on high impact intentional fellowship.", "tokens": [51014, 492, 434, 611, 1293, 16552, 295, 867, 3642, 11, 293, 321, 1116, 411, 281, 2073, 365, 291, 466, 257, 4829, 300, 321, 434, 516, 281, 312, 15578, 412, 264, 7586, 300, 16155, 281, 527, 11500, 1446, 322, 1090, 2712, 21935, 24989, 13, 51814], "temperature": 0.0, "avg_logprob": -0.32639122009277344, "compression_ratio": 1.45, "no_speech_prob": 0.07028607279062271}, {"id": 3, "seek": 2900, "start": 29.0, "end": 44.0, "text": " So what is fellowship, and why is it matter? Well, we spent the last 30 years finding out it away because of the relationship we've enjoyed over all those years, and the more time we spent in fellowship, the more important to the scene to us.", "tokens": [50364, 407, 437, 307, 24989, 11, 293, 983, 307, 309, 1871, 30, 1042, 11, 321, 4418, 264, 1036, 2217, 924, 5006, 484, 309, 1314, 570, 295, 264, 2480, 321, 600, 4626, 670, 439, 729, 924, 11, 293, 264, 544, 565, 321, 4418, 294, 24989, 11, 264, 544, 1021, 281, 264, 4145, 281, 505, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2036649135121128, "compression_ratio": 1.5125, "no_speech_prob": 0.0050033084116876125}, {"id": 4, "seek": 4400, "start": 44.0, "end": 57.0, "text": " In fact, over the past 12 years, we have been actively engaging in that process and developing a way of working together with others that we want to share with you.", "tokens": [50364, 682, 1186, 11, 670, 264, 1791, 2272, 924, 11, 321, 362, 668, 13022, 11268, 294, 300, 1399, 293, 6416, 257, 636, 295, 1364, 1214, 365, 2357, 300, 321, 528, 281, 2073, 365, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06572254080521434, "compression_ratio": 1.3442622950819672, "no_speech_prob": 0.021674344316124916}, {"id": 5, "seek": 5700, "start": 57.0, "end": 70.0, "text": " We're going to be presenting in this conference a number of tools and road maps for how to manage this journey from what we call from the eye to the we to the us.", "tokens": [50364, 492, 434, 516, 281, 312, 15578, 294, 341, 7586, 257, 1230, 295, 3873, 293, 3060, 11317, 337, 577, 281, 3067, 341, 4671, 490, 437, 321, 818, 490, 264, 3313, 281, 264, 321, 281, 264, 505, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13976804312173421, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.12389225512742996}, {"id": 6, "seek": 5700, "start": 70.0, "end": 74.0, "text": " And that is precisely what for us fellowship is about.", "tokens": [51014, 400, 300, 307, 13402, 437, 337, 505, 24989, 307, 466, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13976804312173421, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.12389225512742996}, {"id": 7, "seek": 5700, "start": 74.0, "end": 81.0, "text": " Yeah, because it's very easy to get into a collaboration, if you feel in sync with the person.", "tokens": [51214, 865, 11, 570, 309, 311, 588, 1858, 281, 483, 666, 257, 9363, 11, 498, 291, 841, 294, 20271, 365, 264, 954, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13976804312173421, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.12389225512742996}, {"id": 8, "seek": 8100, "start": 81.0, "end": 99.0, "text": " But what about the larger context where it's not just one plus one, and there are multiple personalities, different stars, different traits, how to find the us that allows for some shared movement together forward, whilst retaining autonomy on self.", "tokens": [50364, 583, 437, 466, 264, 4833, 4319, 689, 309, 311, 406, 445, 472, 1804, 472, 11, 293, 456, 366, 3866, 25308, 11, 819, 6105, 11, 819, 19526, 11, 577, 281, 915, 264, 505, 300, 4045, 337, 512, 5507, 3963, 1214, 2128, 11, 18534, 34936, 27278, 322, 2698, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1310570473764457, "compression_ratio": 1.4910179640718564, "no_speech_prob": 0.02641095034778118}, {"id": 9, "seek": 9900, "start": 99.0, "end": 124.0, "text": " And one of the things that will be presenting here is about how that can, first of all, how to do that, and then how to manage that, how to create ways of continuing that way, I mean, Ian and I have been doing this now for many years, where we meet regularly at least once a week, regardless of where we are in the world and what's going on.", "tokens": [50364, 400, 472, 295, 264, 721, 300, 486, 312, 15578, 510, 307, 466, 577, 300, 393, 11, 700, 295, 439, 11, 577, 281, 360, 300, 11, 293, 550, 577, 281, 3067, 300, 11, 577, 281, 1884, 2098, 295, 9289, 300, 636, 11, 286, 914, 11, 19595, 293, 286, 362, 668, 884, 341, 586, 337, 867, 924, 11, 689, 321, 1677, 11672, 412, 1935, 1564, 257, 1243, 11, 10060, 295, 689, 321, 366, 294, 264, 1002, 293, 437, 311, 516, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10732528141566686, "compression_ratio": 1.6715686274509804, "no_speech_prob": 0.10930584371089935}, {"id": 10, "seek": 12400, "start": 124.0, "end": 144.0, "text": " For us, this is something that is not only enriched our sense of relationship together, it's also improved our productivity, it has been, you know, the source of a number of different products of seminars, articles, books.", "tokens": [50364, 1171, 505, 11, 341, 307, 746, 300, 307, 406, 787, 48624, 527, 2020, 295, 2480, 1214, 11, 309, 311, 611, 9689, 527, 15604, 11, 309, 575, 668, 11, 291, 458, 11, 264, 4009, 295, 257, 1230, 295, 819, 3383, 295, 43112, 11, 11290, 11, 3642, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10711272239685059, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.04547075927257538}, {"id": 11, "seek": 14400, "start": 144.0, "end": 150.0, "text": " So we want to share with you how to do that and why that is so important.", "tokens": [50364, 407, 321, 528, 281, 2073, 365, 291, 577, 281, 360, 300, 293, 983, 300, 307, 370, 1021, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15005760688286324, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.30057790875434875}, {"id": 12, "seek": 14400, "start": 150.0, "end": 158.0, "text": " Now there's a kind of well spring that we've experienced, and we think I didn't work with others, we've seen them experience it as well.", "tokens": [50664, 823, 456, 311, 257, 733, 295, 731, 5587, 300, 321, 600, 6751, 11, 293, 321, 519, 286, 994, 380, 589, 365, 2357, 11, 321, 600, 1612, 552, 1752, 309, 382, 731, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15005760688286324, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.30057790875434875}, {"id": 13, "seek": 14400, "start": 158.0, "end": 164.0, "text": " I hope is that we can take the time we have to give you a taste of that.", "tokens": [51064, 286, 1454, 307, 300, 321, 393, 747, 264, 565, 321, 362, 281, 976, 291, 257, 3939, 295, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15005760688286324, "compression_ratio": 1.5810055865921788, "no_speech_prob": 0.30057790875434875}, {"id": 14, "seek": 16400, "start": 164.0, "end": 188.0, "text": " Is that something that you're kind of curious about, then maybe there's a question to ponder to we meet a little further down the line, which is what would be the pay off for you are being able to make that journey from I to we to us in more contexts with more different kinds of people in different places.", "tokens": [50364, 1119, 300, 746, 300, 291, 434, 733, 295, 6369, 466, 11, 550, 1310, 456, 311, 257, 1168, 281, 280, 8548, 281, 321, 1677, 257, 707, 3052, 760, 264, 1622, 11, 597, 307, 437, 576, 312, 264, 1689, 766, 337, 291, 366, 885, 1075, 281, 652, 300, 4671, 490, 286, 281, 321, 281, 505, 294, 544, 30628, 365, 544, 819, 3685, 295, 561, 294, 819, 3190, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14429032461983818, "compression_ratio": 1.5583756345177664, "no_speech_prob": 0.26466402411460876}, {"id": 15, "seek": 18800, "start": 188.0, "end": 206.0, "text": " And we call this high impact intentional fellowship precisely because it does have a high impact. So kind of be interesting to ponder that impact and then how do we create that we look forward to answering that question at the conference.", "tokens": [50364, 400, 321, 818, 341, 1090, 2712, 21935, 24989, 13402, 570, 309, 775, 362, 257, 1090, 2712, 13, 407, 733, 295, 312, 1880, 281, 280, 8548, 300, 2712, 293, 550, 577, 360, 321, 1884, 300, 321, 574, 2128, 281, 13430, 300, 1168, 412, 264, 7586, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14660468807926885, "compression_ratio": 1.5240963855421688, "no_speech_prob": 0.03757152333855629}, {"id": 16, "seek": 18800, "start": 206.0, "end": 208.0, "text": " See you there.", "tokens": [51264, 3008, 291, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14660468807926885, "compression_ratio": 1.5240963855421688, "no_speech_prob": 0.03757152333855629}], "text": " Hello, I'm Emily Duhm, a final international teaching seminar. I'm Robert Dills, founder of NLP University here in Santa Cruz, California. We're also both authors of many books, and we'd like to share with you about a topic that we're going to be presenting at the conference that relates to our upcoming book on high impact intentional fellowship. So what is fellowship, and why is it matter? Well, we spent the last 30 years finding out it away because of the relationship we've enjoyed over all those years, and the more time we spent in fellowship, the more important to the scene to us. In fact, over the past 12 years, we have been actively engaging in that process and developing a way of working together with others that we want to share with you. We're going to be presenting in this conference a number of tools and road maps for how to manage this journey from what we call from the eye to the we to the us. And that is precisely what for us fellowship is about. Yeah, because it's very easy to get into a collaboration, if you feel in sync with the person. But what about the larger context where it's not just one plus one, and there are multiple personalities, different stars, different traits, how to find the us that allows for some shared movement together forward, whilst retaining autonomy on self. And one of the things that will be presenting here is about how that can, first of all, how to do that, and then how to manage that, how to create ways of continuing that way, I mean, Ian and I have been doing this now for many years, where we meet regularly at least once a week, regardless of where we are in the world and what's going on. For us, this is something that is not only enriched our sense of relationship together, it's also improved our productivity, it has been, you know, the source of a number of different products of seminars, articles, books. So we want to share with you how to do that and why that is so important. Now there's a kind of well spring that we've experienced, and we think I didn't work with others, we've seen them experience it as well. I hope is that we can take the time we have to give you a taste of that. Is that something that you're kind of curious about, then maybe there's a question to ponder to we meet a little further down the line, which is what would be the pay off for you are being able to make that journey from I to we to us in more contexts with more different kinds of people in different places. And we call this high impact intentional fellowship precisely because it does have a high impact. So kind of be interesting to ponder that impact and then how do we create that we look forward to answering that question at the conference. See you there."}
{"video_id": "4f2073Kx4KA", "url": "https://www.youtube.com/watch?v=4f2073Kx4KA", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 16.48, "text": " In the early days of Modeling David Grove, Penny and I attended every workshop he did when", "tokens": [50364, 682, 264, 2440, 1708, 295, 6583, 11031, 4389, 43111, 11, 32009, 293, 286, 15990, 633, 13541, 415, 630, 562, 51188], "temperature": 0.0, "avg_logprob": -0.20598123470942178, "compression_ratio": 1.2907801418439717, "no_speech_prob": 0.1753879189491272}, {"id": 1, "seek": 0, "start": 16.48, "end": 24.16, "text": " he was in the UK. They were usually residential, so we had a lot of access to him. The more", "tokens": [51188, 415, 390, 294, 264, 7051, 13, 814, 645, 2673, 17389, 11, 370, 321, 632, 257, 688, 295, 2105, 281, 796, 13, 440, 544, 51572], "temperature": 0.0, "avg_logprob": -0.20598123470942178, "compression_ratio": 1.2907801418439717, "no_speech_prob": 0.1753879189491272}, {"id": 2, "seek": 2416, "start": 24.16, "end": 30.88, "text": " we showed up, the more he realised just how serious we were about getting good at using his method.", "tokens": [50364, 321, 4712, 493, 11, 264, 544, 415, 21337, 445, 577, 3156, 321, 645, 466, 1242, 665, 412, 1228, 702, 3170, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13429437032560024, "compression_ratio": 1.4912280701754386, "no_speech_prob": 0.017419973388314247}, {"id": 3, "seek": 2416, "start": 32.08, "end": 38.72, "text": " One day, David overheard Penny practicing asking clean language question with another participant.", "tokens": [50760, 1485, 786, 11, 4389, 29807, 515, 32009, 11350, 3365, 2541, 2856, 1168, 365, 1071, 24950, 13, 51092], "temperature": 0.0, "avg_logprob": -0.13429437032560024, "compression_ratio": 1.4912280701754386, "no_speech_prob": 0.017419973388314247}, {"id": 4, "seek": 2416, "start": 39.68, "end": 44.32, "text": " That evening, he asked her if she would like to do some practice with him.", "tokens": [51140, 663, 5634, 11, 415, 2351, 720, 498, 750, 576, 411, 281, 360, 512, 3124, 365, 796, 13, 51372], "temperature": 0.0, "avg_logprob": -0.13429437032560024, "compression_ratio": 1.4912280701754386, "no_speech_prob": 0.017419973388314247}, {"id": 5, "seek": 2416, "start": 45.6, "end": 51.44, "text": " Who's going to say no to that? David and I stood up about 10 feet,", "tokens": [51436, 2102, 311, 516, 281, 584, 572, 281, 300, 30, 4389, 293, 286, 9371, 493, 466, 1266, 3521, 11, 51728], "temperature": 0.0, "avg_logprob": -0.13429437032560024, "compression_ratio": 1.4912280701754386, "no_speech_prob": 0.017419973388314247}, {"id": 6, "seek": 5144, "start": 51.44, "end": 58.559999999999995, "text": " three metres apart, facing each other. Then he said, go on, ask me a question.", "tokens": [50364, 1045, 23861, 4936, 11, 7170, 1184, 661, 13, 1396, 415, 848, 11, 352, 322, 11, 1029, 385, 257, 1168, 13, 50720], "temperature": 0.0, "avg_logprob": -0.12787314022288604, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.002641334431245923}, {"id": 7, "seek": 5144, "start": 59.68, "end": 68.24, "text": " So I asked him, and what would you like to have happen? He said, where do you think that question landed?", "tokens": [50776, 407, 286, 2351, 796, 11, 293, 437, 576, 291, 411, 281, 362, 1051, 30, 634, 848, 11, 689, 360, 291, 519, 300, 1168, 15336, 30, 51204], "temperature": 0.0, "avg_logprob": -0.12787314022288604, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.002641334431245923}, {"id": 8, "seek": 5144, "start": 69.6, "end": 74.8, "text": " Well, probably over there I said waving my hand in his general direction.", "tokens": [51272, 1042, 11, 1391, 670, 456, 286, 848, 35347, 452, 1011, 294, 702, 2674, 3513, 13, 51532], "temperature": 0.0, "avg_logprob": -0.12787314022288604, "compression_ratio": 1.441340782122905, "no_speech_prob": 0.002641334431245923}, {"id": 9, "seek": 7480, "start": 75.2, "end": 84.16, "text": " No, he said that question fell somewhere in the space between us. It never reached me.", "tokens": [50384, 883, 11, 415, 848, 300, 1168, 5696, 4079, 294, 264, 1901, 1296, 505, 13, 467, 1128, 6488, 385, 13, 50832], "temperature": 0.0, "avg_logprob": -0.21722697004487243, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.004692601505666971}, {"id": 10, "seek": 7480, "start": 84.16, "end": 93.52, "text": " Ask it again only this time with Oomph, and he added, if you don't got Oomph, you don't got it.", "tokens": [50832, 12320, 309, 797, 787, 341, 565, 365, 422, 298, 950, 11, 293, 415, 3869, 11, 498, 291, 500, 380, 658, 422, 298, 950, 11, 291, 500, 380, 658, 309, 13, 51300], "temperature": 0.0, "avg_logprob": -0.21722697004487243, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.004692601505666971}, {"id": 11, "seek": 7480, "start": 93.52, "end": 104.32, "text": " So I had another go. We did this over and over until I learned to ask my questions with my whole", "tokens": [51300, 407, 286, 632, 1071, 352, 13, 492, 630, 341, 670, 293, 670, 1826, 286, 3264, 281, 1029, 452, 1651, 365, 452, 1379, 51840], "temperature": 0.0, "avg_logprob": -0.21722697004487243, "compression_ratio": 1.4919786096256684, "no_speech_prob": 0.004692601505666971}, {"id": 12, "seek": 10432, "start": 104.32, "end": 113.6, "text": " torso, not just from the top part of my chest and throat. David also said once, I could deliver a", "tokens": [50364, 34917, 11, 406, 445, 490, 264, 1192, 644, 295, 452, 7443, 293, 12394, 13, 4389, 611, 848, 1564, 11, 286, 727, 4239, 257, 50828], "temperature": 0.0, "avg_logprob": -0.10335579043940495, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0008547664037905633}, {"id": 13, "seek": 10432, "start": 113.6, "end": 121.19999999999999, "text": " question with Oomph, I needed to pay attention to where my question landed in the client's perceptual", "tokens": [50828, 1168, 365, 422, 298, 950, 11, 286, 2978, 281, 1689, 3202, 281, 689, 452, 1168, 15336, 294, 264, 6423, 311, 43276, 901, 51208], "temperature": 0.0, "avg_logprob": -0.10335579043940495, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0008547664037905633}, {"id": 14, "seek": 10432, "start": 121.19999999999999, "end": 131.6, "text": " space. To do this, I needed to know where the client perceived their symbols to be, in their space,", "tokens": [51208, 1901, 13, 1407, 360, 341, 11, 286, 2978, 281, 458, 689, 264, 6423, 19049, 641, 16944, 281, 312, 11, 294, 641, 1901, 11, 51728], "temperature": 0.0, "avg_logprob": -0.10335579043940495, "compression_ratio": 1.5654450261780104, "no_speech_prob": 0.0008547664037905633}, {"id": 15, "seek": 13160, "start": 131.6, "end": 139.84, "text": " and to deliver my question to that particular space. Again, practice was needed, and when I", "tokens": [50364, 293, 281, 4239, 452, 1168, 281, 300, 1729, 1901, 13, 3764, 11, 3124, 390, 2978, 11, 293, 562, 286, 50776], "temperature": 0.0, "avg_logprob": -0.07311519678088202, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.0003617988550104201}, {"id": 16, "seek": 13160, "start": 139.84, "end": 147.12, "text": " finally got it, it was like I was an archer that could fire an arrow and hit the bull's eye.", "tokens": [50776, 2721, 658, 309, 11, 309, 390, 411, 286, 390, 364, 3912, 260, 300, 727, 2610, 364, 11610, 293, 2045, 264, 4693, 311, 3313, 13, 51140], "temperature": 0.0, "avg_logprob": -0.07311519678088202, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.0003617988550104201}, {"id": 17, "seek": 13160, "start": 148.48, "end": 155.6, "text": " This taught me that my attention needed to be over there in the client's metaphor landscape.", "tokens": [51208, 639, 5928, 385, 300, 452, 3202, 2978, 281, 312, 670, 456, 294, 264, 6423, 311, 19157, 9661, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07311519678088202, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.0003617988550104201}, {"id": 18, "seek": 15560, "start": 155.92, "end": 163.6, "text": " Learning to deliver questions like this transformed the effectiveness of every future", "tokens": [50380, 15205, 281, 4239, 1651, 411, 341, 16894, 264, 21208, 295, 633, 2027, 50764], "temperature": 0.0, "avg_logprob": -0.17074663597240783, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.03761984407901764}, {"id": 19, "seek": 15560, "start": 163.6, "end": 172.24, "text": " clean language question I ever asked. So instead of thinking, what question should I ask next?", "tokens": [50764, 2541, 2856, 1168, 286, 1562, 2351, 13, 407, 2602, 295, 1953, 11, 437, 1168, 820, 286, 1029, 958, 30, 51196], "temperature": 0.0, "avg_logprob": -0.17074663597240783, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.03761984407901764}, {"id": 20, "seek": 15560, "start": 173.04, "end": 181.2, "text": " We first wonder what and where in the metaphor landscape would it be useful for the client to", "tokens": [51236, 492, 700, 2441, 437, 293, 689, 294, 264, 19157, 9661, 576, 309, 312, 4420, 337, 264, 6423, 281, 51644], "temperature": 0.0, "avg_logprob": -0.17074663597240783, "compression_ratio": 1.4891304347826086, "no_speech_prob": 0.03761984407901764}, {"id": 21, "seek": 18120, "start": 181.2, "end": 187.2, "text": " attend to? Once we know that, choosing the appropriate clean language question is easy.", "tokens": [50364, 6888, 281, 30, 3443, 321, 458, 300, 11, 10875, 264, 6854, 2541, 2856, 1168, 307, 1858, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20484936454079367, "compression_ratio": 1.4566473988439306, "no_speech_prob": 0.0023724157363176346}, {"id": 22, "seek": 18120, "start": 188.64, "end": 196.88, "text": " So join us for our Masterclass at the NLP conference in London and discover more nuggets about how", "tokens": [50736, 407, 3917, 505, 337, 527, 6140, 11665, 412, 264, 426, 45196, 7586, 294, 7042, 293, 4411, 544, 42663, 466, 577, 51148], "temperature": 0.0, "avg_logprob": -0.20484936454079367, "compression_ratio": 1.4566473988439306, "no_speech_prob": 0.0023724157363176346}, {"id": 23, "seek": 18120, "start": 196.88, "end": 203.35999999999999, "text": " to skillfully use clean language questions and symbolic modeling.", "tokens": [51148, 281, 5389, 2277, 764, 2541, 2856, 1651, 293, 25755, 15983, 13, 51472], "temperature": 0.0, "avg_logprob": -0.20484936454079367, "compression_ratio": 1.4566473988439306, "no_speech_prob": 0.0023724157363176346}], "text": " In the early days of Modeling David Grove, Penny and I attended every workshop he did when he was in the UK. They were usually residential, so we had a lot of access to him. The more we showed up, the more he realised just how serious we were about getting good at using his method. One day, David overheard Penny practicing asking clean language question with another participant. That evening, he asked her if she would like to do some practice with him. Who's going to say no to that? David and I stood up about 10 feet, three metres apart, facing each other. Then he said, go on, ask me a question. So I asked him, and what would you like to have happen? He said, where do you think that question landed? Well, probably over there I said waving my hand in his general direction. No, he said that question fell somewhere in the space between us. It never reached me. Ask it again only this time with Oomph, and he added, if you don't got Oomph, you don't got it. So I had another go. We did this over and over until I learned to ask my questions with my whole torso, not just from the top part of my chest and throat. David also said once, I could deliver a question with Oomph, I needed to pay attention to where my question landed in the client's perceptual space. To do this, I needed to know where the client perceived their symbols to be, in their space, and to deliver my question to that particular space. Again, practice was needed, and when I finally got it, it was like I was an archer that could fire an arrow and hit the bull's eye. This taught me that my attention needed to be over there in the client's metaphor landscape. Learning to deliver questions like this transformed the effectiveness of every future clean language question I ever asked. So instead of thinking, what question should I ask next? We first wonder what and where in the metaphor landscape would it be useful for the client to attend to? Once we know that, choosing the appropriate clean language question is easy. So join us for our Masterclass at the NLP conference in London and discover more nuggets about how to skillfully use clean language questions and symbolic modeling."}
{"video_id": "P3icxlpgPhE", "url": "https://www.youtube.com/watch?v=P3icxlpgPhE", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.16, "text": " David Grove used to say all questions are up to something.", "tokens": [50364, 4389, 43111, 1143, 281, 584, 439, 1651, 366, 493, 281, 746, 13, 51122], "temperature": 0.0, "avg_logprob": -0.1754295539855957, "compression_ratio": 1.5612903225806452, "no_speech_prob": 0.23469649255275726}, {"id": 1, "seek": 0, "start": 15.16, "end": 22.32, "text": " The distinction between leading and clean questions is they are up to different things.", "tokens": [51122, 440, 16844, 1296, 5775, 293, 2541, 1651, 307, 436, 366, 493, 281, 819, 721, 13, 51480], "temperature": 0.0, "avg_logprob": -0.1754295539855957, "compression_ratio": 1.5612903225806452, "no_speech_prob": 0.23469649255275726}, {"id": 2, "seek": 0, "start": 22.32, "end": 28.96, "text": " The vast majority of questions people ask whether in everyday conversation or in a professional", "tokens": [51480, 440, 8369, 6286, 295, 1651, 561, 1029, 1968, 294, 7429, 3761, 420, 294, 257, 4843, 51812], "temperature": 0.0, "avg_logprob": -0.1754295539855957, "compression_ratio": 1.5612903225806452, "no_speech_prob": 0.23469649255275726}, {"id": 3, "seek": 2896, "start": 28.96, "end": 37.88, "text": " setting are leading. A new book, Clean Language Interviewing, goes into great detail about", "tokens": [50364, 3287, 366, 5775, 13, 316, 777, 1446, 11, 18463, 24445, 35599, 278, 11, 1709, 666, 869, 2607, 466, 50810], "temperature": 0.0, "avg_logprob": -0.17982165018717447, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0013852582778781652}, {"id": 4, "seek": 2896, "start": 37.88, "end": 42.4, "text": " the difference between the two types of question.", "tokens": [50810, 264, 2649, 1296, 264, 732, 3467, 295, 1168, 13, 51036], "temperature": 0.0, "avg_logprob": -0.17982165018717447, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0013852582778781652}, {"id": 5, "seek": 2896, "start": 42.4, "end": 50.5, "text": " David Grove spent 25 years looking for clean, non-leading questions. In all that time he", "tokens": [51036, 4389, 43111, 4418, 3552, 924, 1237, 337, 2541, 11, 2107, 12, 28012, 1651, 13, 682, 439, 300, 565, 415, 51441], "temperature": 0.0, "avg_logprob": -0.17982165018717447, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0013852582778781652}, {"id": 6, "seek": 2896, "start": 50.5, "end": 58.480000000000004, "text": " found less than a dozen that are universally clean. That is, they can be asked in almost", "tokens": [51441, 1352, 1570, 813, 257, 16654, 300, 366, 43995, 2541, 13, 663, 307, 11, 436, 393, 312, 2351, 294, 1920, 51840], "temperature": 0.0, "avg_logprob": -0.17982165018717447, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.0013852582778781652}, {"id": 7, "seek": 5848, "start": 58.48, "end": 63.76, "text": " any context including therapy and coaching.", "tokens": [50364, 604, 4319, 3009, 9492, 293, 15818, 13, 50628], "temperature": 0.0, "avg_logprob": -0.20872657399781994, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.005781044717878103}, {"id": 8, "seek": 5848, "start": 63.76, "end": 72.28, "text": " So what makes a clean language question clean? And why are there so few of them? Despite", "tokens": [50628, 407, 437, 1669, 257, 2541, 2856, 1168, 2541, 30, 400, 983, 366, 456, 370, 1326, 295, 552, 30, 11334, 51054], "temperature": 0.0, "avg_logprob": -0.20872657399781994, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.005781044717878103}, {"id": 9, "seek": 5848, "start": 72.28, "end": 79.72, "text": " what followers of Karl Rogers might think there's no such thing as a non-directive question.", "tokens": [51054, 437, 13071, 295, 20405, 29877, 1062, 519, 456, 311, 572, 1270, 551, 382, 257, 2107, 12, 44868, 488, 1168, 13, 51426], "temperature": 0.0, "avg_logprob": -0.20872657399781994, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.005781044717878103}, {"id": 10, "seek": 5848, "start": 79.72, "end": 86.72, "text": " All questions are directive, even clean language questions. In that every question invites", "tokens": [51426, 1057, 1651, 366, 45444, 11, 754, 2541, 2856, 1651, 13, 682, 300, 633, 1168, 35719, 51776], "temperature": 0.0, "avg_logprob": -0.20872657399781994, "compression_ratio": 1.6205128205128205, "no_speech_prob": 0.005781044717878103}, {"id": 11, "seek": 8672, "start": 86.72, "end": 92.08, "text": " the client to consider one thing rather than another.", "tokens": [50364, 264, 6423, 281, 1949, 472, 551, 2831, 813, 1071, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1441025250199912, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.00436925794929266}, {"id": 12, "seek": 8672, "start": 92.08, "end": 99.92, "text": " Clean language questions operate at a process level. They do not introduce any content. They", "tokens": [50632, 18463, 2856, 1651, 9651, 412, 257, 1399, 1496, 13, 814, 360, 406, 5366, 604, 2701, 13, 814, 51024], "temperature": 0.0, "avg_logprob": -0.1441025250199912, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.00436925794929266}, {"id": 13, "seek": 8672, "start": 99.92, "end": 106.72, "text": " only incorporate the information that's already been supplied by the client using their", "tokens": [51024, 787, 16091, 264, 1589, 300, 311, 1217, 668, 27625, 538, 264, 6423, 1228, 641, 51364], "temperature": 0.0, "avg_logprob": -0.1441025250199912, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.00436925794929266}, {"id": 14, "seek": 8672, "start": 106.72, "end": 114.68, "text": " exact words. All the other words in the question are process words that are short, simple", "tokens": [51364, 1900, 2283, 13, 1057, 264, 661, 2283, 294, 264, 1168, 366, 1399, 2283, 300, 366, 2099, 11, 2199, 51762], "temperature": 0.0, "avg_logprob": -0.1441025250199912, "compression_ratio": 1.6281407035175879, "no_speech_prob": 0.00436925794929266}, {"id": 15, "seek": 11468, "start": 114.68, "end": 121.52000000000001, "text": " and relate to fundamental ideas about space, time and matter.", "tokens": [50364, 293, 10961, 281, 8088, 3487, 466, 1901, 11, 565, 293, 1871, 13, 50706], "temperature": 0.0, "avg_logprob": -0.13913673673357282, "compression_ratio": 1.642156862745098, "no_speech_prob": 0.008310262113809586}, {"id": 16, "seek": 11468, "start": 121.52000000000001, "end": 127.76, "text": " The reason there are so few questions that meet this criteria is because there is a surprisingly", "tokens": [50706, 440, 1778, 456, 366, 370, 1326, 1651, 300, 1677, 341, 11101, 307, 570, 456, 307, 257, 17600, 51018], "temperature": 0.0, "avg_logprob": -0.13913673673357282, "compression_ratio": 1.642156862745098, "no_speech_prob": 0.008310262113809586}, {"id": 17, "seek": 11468, "start": 127.76, "end": 136.36, "text": " small number of universal concepts that exist in every language studied so far. These", "tokens": [51018, 1359, 1230, 295, 11455, 10392, 300, 2514, 294, 633, 2856, 9454, 370, 1400, 13, 1981, 51448], "temperature": 0.0, "avg_logprob": -0.13913673673357282, "compression_ratio": 1.642156862745098, "no_speech_prob": 0.008310262113809586}, {"id": 18, "seek": 11468, "start": 136.36, "end": 144.56, "text": " universal concepts are called semantic primes and they form the basis of the classic clean", "tokens": [51448, 11455, 10392, 366, 1219, 47982, 582, 1532, 293, 436, 1254, 264, 5143, 295, 264, 7230, 2541, 51858], "temperature": 0.0, "avg_logprob": -0.13913673673357282, "compression_ratio": 1.642156862745098, "no_speech_prob": 0.008310262113809586}, {"id": 19, "seek": 14456, "start": 144.56, "end": 159.6, "text": " language questions such as, and is there anything else about? And what kind of is that?", "tokens": [50364, 2856, 1651, 1270, 382, 11, 293, 307, 456, 1340, 1646, 466, 30, 400, 437, 733, 295, 307, 300, 30, 51116], "temperature": 0.0, "avg_logprob": -0.15929290983412, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0016045296797528863}, {"id": 20, "seek": 14456, "start": 159.6, "end": 168.04, "text": " And where is? But there's more to a clean language question than the words. There's also", "tokens": [51116, 400, 689, 307, 30, 583, 456, 311, 544, 281, 257, 2541, 2856, 1168, 813, 264, 2283, 13, 821, 311, 611, 51538], "temperature": 0.0, "avg_logprob": -0.15929290983412, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.0016045296797528863}, {"id": 21, "seek": 16804, "start": 168.04, "end": 174.44, "text": " the purpose or intention of a clean question and the way it is asked.", "tokens": [50364, 264, 4334, 420, 7789, 295, 257, 2541, 1168, 293, 264, 636, 309, 307, 2351, 13, 50684], "temperature": 0.0, "avg_logprob": -0.1174066519435448, "compression_ratio": 1.7914438502673797, "no_speech_prob": 0.12790246307849884}, {"id": 22, "seek": 16804, "start": 174.44, "end": 181.92, "text": " Clean language questions are designed to send a client on a quest for self-knowledge. We", "tokens": [50684, 18463, 2856, 1651, 366, 4761, 281, 2845, 257, 6423, 322, 257, 866, 337, 2698, 12, 15869, 3042, 13, 492, 51058], "temperature": 0.0, "avg_logprob": -0.1174066519435448, "compression_ratio": 1.7914438502673797, "no_speech_prob": 0.12790246307849884}, {"id": 23, "seek": 16804, "start": 181.92, "end": 189.35999999999999, "text": " call this the client self-modeling. Even when a facilitator feels they need to know the", "tokens": [51058, 818, 341, 264, 6423, 2698, 12, 8014, 11031, 13, 2754, 562, 257, 38770, 1639, 3417, 436, 643, 281, 458, 264, 51430], "temperature": 0.0, "avg_logprob": -0.1174066519435448, "compression_ratio": 1.7914438502673797, "no_speech_prob": 0.12790246307849884}, {"id": 24, "seek": 16804, "start": 189.35999999999999, "end": 195.23999999999998, "text": " answer to a question, clean language questions are designed so that the client discovers", "tokens": [51430, 1867, 281, 257, 1168, 11, 2541, 2856, 1651, 366, 4761, 370, 300, 264, 6423, 44522, 51724], "temperature": 0.0, "avg_logprob": -0.1174066519435448, "compression_ratio": 1.7914438502673797, "no_speech_prob": 0.12790246307849884}, {"id": 25, "seek": 19524, "start": 195.24, "end": 203.96, "text": " the information for themselves first and only then if they wish shares it with the facilitator.", "tokens": [50364, 264, 1589, 337, 2969, 700, 293, 787, 550, 498, 436, 3172, 12182, 309, 365, 264, 38770, 1639, 13, 50800], "temperature": 0.0, "avg_logprob": -0.26002236018105157, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.10070487856864929}, {"id": 26, "seek": 19524, "start": 203.96, "end": 212.92000000000002, "text": " As David Grove used to say, clean language is deeply agreeable to the client's heart and", "tokens": [50800, 1018, 4389, 43111, 1143, 281, 584, 11, 2541, 2856, 307, 8760, 3986, 712, 281, 264, 6423, 311, 1917, 293, 51248], "temperature": 0.0, "avg_logprob": -0.26002236018105157, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.10070487856864929}, {"id": 27, "seek": 19524, "start": 212.92000000000002, "end": 222.84, "text": " soul. A list of the commonly asked clean questions, including translations in over 20 languages,", "tokens": [51248, 5133, 13, 316, 1329, 295, 264, 12719, 2351, 2541, 1651, 11, 3009, 37578, 294, 670, 945, 8650, 11, 51744], "temperature": 0.0, "avg_logprob": -0.26002236018105157, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.10070487856864929}, {"id": 28, "seek": 22284, "start": 222.84, "end": 230.88, "text": " are freely available on our website cleanlanguage.com. We'd love to have you join us at our", "tokens": [50364, 366, 16433, 2435, 322, 527, 3144, 2541, 25241, 20473, 13, 1112, 13, 492, 1116, 959, 281, 362, 291, 3917, 505, 412, 527, 50766], "temperature": 0.0, "avg_logprob": -0.2701343854268392, "compression_ratio": 1.1304347826086956, "no_speech_prob": 0.13549242913722992}, {"id": 29, "seek": 22284, "start": 230.88, "end": 232.04, "text": " Masterclass.", "tokens": [50766, 6140, 11665, 13, 50824], "temperature": 0.0, "avg_logprob": -0.2701343854268392, "compression_ratio": 1.1304347826086956, "no_speech_prob": 0.13549242913722992}], "text": " David Grove used to say all questions are up to something. The distinction between leading and clean questions is they are up to different things. The vast majority of questions people ask whether in everyday conversation or in a professional setting are leading. A new book, Clean Language Interviewing, goes into great detail about the difference between the two types of question. David Grove spent 25 years looking for clean, non-leading questions. In all that time he found less than a dozen that are universally clean. That is, they can be asked in almost any context including therapy and coaching. So what makes a clean language question clean? And why are there so few of them? Despite what followers of Karl Rogers might think there's no such thing as a non-directive question. All questions are directive, even clean language questions. In that every question invites the client to consider one thing rather than another. Clean language questions operate at a process level. They do not introduce any content. They only incorporate the information that's already been supplied by the client using their exact words. All the other words in the question are process words that are short, simple and relate to fundamental ideas about space, time and matter. The reason there are so few questions that meet this criteria is because there is a surprisingly small number of universal concepts that exist in every language studied so far. These universal concepts are called semantic primes and they form the basis of the classic clean language questions such as, and is there anything else about? And what kind of is that? And where is? But there's more to a clean language question than the words. There's also the purpose or intention of a clean question and the way it is asked. Clean language questions are designed to send a client on a quest for self-knowledge. We call this the client self-modeling. Even when a facilitator feels they need to know the answer to a question, clean language questions are designed so that the client discovers the information for themselves first and only then if they wish shares it with the facilitator. As David Grove used to say, clean language is deeply agreeable to the client's heart and soul. A list of the commonly asked clean questions, including translations in over 20 languages, are freely available on our website cleanlanguage.com. We'd love to have you join us at our Masterclass."}
